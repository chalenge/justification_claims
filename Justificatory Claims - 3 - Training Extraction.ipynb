{
 "metadata": {
  "name": "",
  "signature": "sha256:a34ba49c9425abfa6f0a4d7ca4f3bc9ed0058ccbabf03e7649e10cd9f5fd8bb1"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#Training Extraction#"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Import statments:\n",
      "import csv\n",
      "import itertools\n",
      "import nltk\n",
      "import random\n",
      "from __future__ import division\n",
      "import time\n",
      "import cPickle as pickle \n",
      "import json\n",
      "import math\n",
      "import numpy\n",
      "from sklearn import cross_validation\n",
      "from collections import Counter\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "training_data = 'training3.p'\n",
      "hand_coded_devtest_file = 'tagged_data.json'"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "###Import from Pickle###"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# This loads the training and dictionary from disk.\n",
      "# This dictionaries contain messages with sentences tokenized and POS tagged.\n",
      "with open(training_data, 'rb') as fp:\n",
      "    training = pickle.load(fp)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 447
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Use Cue Phrases to Extract Text Spans\n",
      "#### cpp = cue phrase pattern, bn = between span pattern, wn = within span pattern\n",
      " - cpp_bn_so:\n",
      "     - [BOS justification EOS] [BOS So, claim EOS]\n",
      " - cpp_wn_per:\n",
      "     - [BOS claim], [per justification EOS]\n",
      " - cpp_bn_becauseofthis:\n",
      "     - [BOS justification EOS] [BOS Because of this, claim EOS]\n",
      " - cpp_wn_because1:\n",
      "     - [BOS Because justification, claim EOS]\n",
      " - cpp_wn_because2:\n",
      "     - [BOS claim, because justification EOS]\n",
      " - cpp_wn_because3\n",
      "     - [BOS claim because justification EOS]\n",
      " - cpp_wn_since:\n",
      "     - [BOS Since justification, claim EOS]\n",
      " - cpp_bn_therefore:\n",
      "     - [BOS justification EOS] [BOS Therefore, claim EOS]\n",
      " - cpp_bn_consequently:\n",
      "     - [BOS justification EOS] [BOS Consequently, claim EOS]\n",
      " - cue_signal_unclassified:\n",
      "     - Has cue word but does not follow cue phrase pattern\n",
      "     - This is extracted for development purpose, later combined with no_relation for classification purposes\n",
      " - no_relation:\n",
      "     - No cue word"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "'''\n",
      "Iterates over all mail messages, then checks if it contains a cue phrase. \n",
      "It then builds a text span pair which could include the previous sentence or two parts of\n",
      "the current sentence depending on the position of the cue phrase.\n",
      "This text span will not include the cue signal.\n",
      "'''\n",
      "mail_spans = {}\n",
      "cue_phrases = ['so', 'because', 'since', 'per', 'thus', 'therefore', 'consequently']\n",
      "for key, message in training.items():\n",
      "    for sent_num in range(0,len(message)): # Iterate through each sent in message\n",
      "        sentence = message[sent_num]\n",
      "\n",
      "        # If ZERO words in sentence are in cue_phrases, sent to no_relation\n",
      "        if len([word for (word, pos) in sentence if word in cue_phrases]) == 0:\n",
      "            phrase_key = 'no_relation'\n",
      "            if phrase_key in mail_spans:\n",
      "                mail_spans[phrase_key].append((message[sent_num-1],sentence))\n",
      "            else:\n",
      "                mail_spans[phrase_key] = [(message[sent_num-1],sentence)]\n",
      "                \n",
      "        for word_num in range(0,len(sentence)): # Iterate through each word in sent\n",
      "            split = 0 # Default, if unchanged it signals cue signal was present but no cue phrase pattern\n",
      "            word = sentence[word_num][0].lower() \n",
      "\n",
      "            if word_num != 0:\n",
      "                prev_word = sentence[word_num-1][0].lower()\n",
      "            else: \n",
      "                prev_word = ''\n",
      "                \n",
      "            if word_num != (len(sentence)-1):\n",
      "                next_word = sentence[word_num+1][0].lower()\n",
      "            else:\n",
      "                next_word = ''\n",
      "\n",
      "            if word in cue_phrases:                \n",
      "                '''\n",
      "                Pull out the text phrase based on the cue phrase. Probably a \n",
      "                continuation of last sentence or split the sentence at the last \n",
      "                position of the cue phrases\n",
      "                '''\n",
      "                if sentence[-1][0] != '?':\n",
      "                    # cpp_bn_so\n",
      "                    if word_num == 0 and word == 'so':\n",
      "                        if next_word == ',':\n",
      "                            split = range(2)\n",
      "                            phrase_key = 'cpp_bn_so'\n",
      "                    # cpp_wn_per\n",
      "                    elif word_num != 0 and word == 'per':\n",
      "                        if prev_word == ',' and next_word != '=' and next_word != 'se':\n",
      "                            split = 'oneprior'\n",
      "                            phrase_key = 'cpp_wn_per'\n",
      "                    # cpp_bn_becauseofthis\n",
      "                    elif word_num == 0 and word == 'because':\n",
      "                        if ((next_word == 'of') and (sentence[word_num+2][0] == 'this')) and (sentence[word_num+3][0] == ','):\n",
      "                            split = range(4)\n",
      "                            phrase_key = 'cpp_bn_becauseofthis'\n",
      "                    # cpp_wn_because1\n",
      "                        elif (',', ',') in sentence:\n",
      "                              split = sentence.index((',', ','))\n",
      "                              phrase_key = 'cpp_wn_because1'\n",
      "                    # cpp_wn_because2\n",
      "                    elif word_num != 0 and word == 'because':\n",
      "                        if prev_word == ',':\n",
      "                            split = 'oneprior'\n",
      "                            phrase_key = 'cpp_wn_because2'\n",
      "                    # cpp_wn_because3\n",
      "                        elif word_num != len(sentence):\n",
      "                            split = 'exact'\n",
      "                            phrase_key = 'cpp_wn_because3'\n",
      "\n",
      "\n",
      "                    # cpp_wn_since\n",
      "                    elif word_num == 0 and word == 'since':\n",
      "                        if (',', ',') in sentence:\n",
      "                            split = sentence.index((',', ','))\n",
      "                            phrase_key = 'cpp_wn_since'\n",
      "                    # cpp_bn_therefore\n",
      "                    elif word_num == 0 and word == 'therefore':\n",
      "                        if next_word == ',':\n",
      "                            split = range(2)\n",
      "                            phrase_key = 'cpp_bn_therefore'\n",
      "                    # cpp_bn_consequently\n",
      "                    elif word_num == 0 and word == 'consequently':\n",
      "                        if next_word == ',':\n",
      "                            split = range(2)\n",
      "                            phrase_key = 'cpp_bn_consequently'  \n",
      "                \n",
      "                \n",
      "                '''\n",
      "                The code below splits the sentence pairs into text spans or splits the sentence itself \n",
      "                AND removes cue signals\n",
      "                '''\n",
      "                if split == 0:\n",
      "                    # Has cue word but does not follow cue phrase pattern\n",
      "                    phrase_key = 'cue_signal_unclassified'\n",
      "                    text_span = (message[sent_num-1],sentence)\n",
      "                    \n",
      "                if phrase_key[:6] == 'cpp_bn':\n",
      "                    if sent_num == 0:\n",
      "                        # Removes 21 mistaken occurences where the cue phrase extracts\n",
      "                        # the first sentence in a message as holding a discourse relation\n",
      "                        # with the sentence before it, something our classifier is not\n",
      "                        # to handle.\n",
      "                        phrase_key = 'cue_signal_unclassified'\n",
      "                        text_span = (message[sent_num-1],sentence)\n",
      "                    elif split == range(2):\n",
      "                        text_span = (message[sent_num-1],sentence)\n",
      "                    elif split == range(4):\n",
      "                        text_span = (message[sent_num-1],sentence)\n",
      "                elif phrase_key[:6] == 'cpp_wn':\n",
      "                    if split == 'oneprior':\n",
      "                        text_span = (sentence[:word_num-1],sentence[word_num-1:])\n",
      "                    elif split == 'exact':\n",
      "                        text_span = (sentence[:word_num],sentence[word_num:])\n",
      "                    else: # Ex. Because justification, claim. OR Since justification, claim. split = index of comma\n",
      "                        text_span = (sentence[:split],sentence[split:])\n",
      "\n",
      "                # Store the spans in mail_spans\n",
      "                if phrase_key in mail_spans:\n",
      "                    mail_spans[phrase_key].append(text_span)\n",
      "                else:\n",
      "                    mail_spans[phrase_key] = [text_span]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 448
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Check counts:\n",
      "for cue_phrase, items in sorted(mail_spans.items()):  \n",
      "    print 'Number of %s: %d' % (cue_phrase, len(items))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Number of cpp_bn_consequently: 5\n",
        "Number of cpp_bn_so: 20\n",
        "Number of cpp_bn_therefore: 8\n",
        "Number of cpp_wn_because1: 9\n",
        "Number of cpp_wn_because2: 37\n",
        "Number of cpp_wn_because3: 131\n",
        "Number of cpp_wn_per: 5\n",
        "Number of cpp_wn_since: 26\n",
        "Number of cue_signal_unclassified: 833\n",
        "Number of no_relation: 15260\n"
       ]
      }
     ],
     "prompt_number": 449
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Use for examining various cue_phrase extractions:\n",
      "for textspan1, textspan2 in mail_spans['cue_signal_unclassified']:\n",
      "    print 'Textspan 1:'+' '.join([w for w, pos in textspan1])\n",
      "    print 'Textspan 2:'+' '.join([w for w, pos in textspan2])\n",
      "    print"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "count = 0\n",
      "to_tag = list()\n",
      "for cue_phrase, items in sorted(mail_spans.items())[:9]:  \n",
      "    for item in items:\n",
      "        dict_ = dict()\n",
      "        dict_['untagged'] = '__SENT_1__'+' '.join([x[0] for x in item[0]])+' __SENT_2__ '+' '.join([x[0] for x in item[1]])\n",
      "        dict_['sent'] =  'one', item[0]\n",
      "        dict_['prev'] =  'two', item[1]\n",
      "        to_tag.append(dict_)\n",
      "for x in to_tag[:1]:\n",
      "    for y in x.items():\n",
      "        print y\n",
      "        print"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "('untagged', '__SENT_1__However , this volume of scientific production has focused excessively on the English version of Wikipedia when Wikipedia is now available in 279 different languages . __SENT_2__ Consequently , the current bibliography does not pay sufficient attention to the dynamics and peculiarities of versions of Wikipedia in other languages , which makes a comparative analysis showing the contrasts and similarities between the different communities difficult .')\n",
        "\n",
        "('prev', ('two', [('Consequently', u'RB'), (',', u','), ('the', u'AT'), ('current', u'JJ'), ('bibliography', 'NN'), ('does', u'DOZ'), ('not', u'*'), ('pay', u'VB'), ('sufficient', u'JJ'), ('attention', 'NN'), ('to', u'IN'), ('the', u'AT'), ('dynamics', u'NNS'), ('and', u'CC'), ('peculiarities', u'NNS'), ('of', u'IN'), ('versions', u'NNS'), ('of', u'IN'), ('Wikipedia', 'NN'), ('in', u'IN'), ('other', u'AP'), ('languages', u'NNS'), (',', u','), ('which', u'WDT'), ('makes', u'VBZ'), ('a', u'AT'), ('comparative', u'JJ'), ('analysis', u'NN'), ('showing', u'VBG'), ('the', u'AT'), ('contrasts', u'NNS'), ('and', u'CC'), ('similarities', u'NNS'), ('between', u'IN'), ('the', u'AT'), ('different', u'JJ'), ('communities', u'NNS'), ('difficult', u'JJ'), ('.', u'.')]))\n",
        "\n",
        "('sent', ('one', [('However', u'RB'), (',', u','), ('this', u'DT'), ('volume', u'NN'), ('of', u'IN'), ('scientific', u'JJ'), ('production', 'NN'), ('has', u'HVZ'), ('focused', u'VBN'), ('excessively', u'QL'), ('on', u'IN'), ('the', u'AT'), ('English', u'JJ'), ('version', 'NN'), ('of', u'IN'), ('Wikipedia', 'NN'), ('when', u'WRB'), ('Wikipedia', 'NN'), ('is', u'BEZ'), ('now', u'RB'), ('available', u'JJ'), ('in', u'IN'), ('279', 'NN'), ('different', u'JJ'), ('languages', u'NNS'), ('.', u'.')]))\n",
        "\n"
       ]
      }
     ],
     "prompt_number": 451
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Save to JSON for hand-coding.\n",
      "with open('../to_tag.json', 'w+b') as jfile:\n",
      "    json.dump(to_tag, jfile, ensure_ascii=False)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 452
    }
   ],
   "metadata": {}
  }
 ]
}