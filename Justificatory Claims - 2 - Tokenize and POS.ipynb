{
 "metadata": {
  "name": "",
  "signature": "sha256:ffb3de4ae72990ab7921b0be5e39364f3d2193aa5834d519b655d6a4b78d56b9"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "###This reads from .csv:###"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import csv\n",
      "import sys\n",
      "csv.field_size_limit(sys.maxsize) # Again, some size is too large (ref: http://stackoverflow.com/questions/15063936/csv-error-field-larger-than-field-limit-131072)\n",
      "with open('Justificatory_Claims_raw.csv', mode='r') as fin:\n",
      "    reader = csv.reader(fin)\n",
      "    data_dict = {rows[0]:rows[1] for rows in reader} # ref: http://stackoverflow.com/questions/6740918/creating-a-dictionary-from-a-csv-file"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print 'Mailing lists:', 'ipython-dev, ipython-user, scipy-dev, scipy-user'\n",
      "print \"# of messages:\", len(data_dict)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "###Use the below to take a look at random messages:###"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import random\n",
      "num = random.randint(0, len(data_dict))\n",
      "print 'LOOKING AT MESSAGE %s:' % num\n",
      "print\n",
      "for key in data_dict.keys()[num:num+1]:\n",
      "    print data_dict[key]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "###Use the below to take a look at random messages matching a simplistic search string:###"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "search_string = 'because' #Examples: beacuse, since, although, thus, therefore\n",
      "search_results = list()\n",
      "for key in data_dict.keys():\n",
      "    if search_string in data_dict[key]:\n",
      "        search_results.append(data_dict[key])\n",
      "num = random.randint(0, len(search_results))\n",
      "\n",
      "print 'LOOKING AT SEARCH RESULT FOR \"%s\" OF %s RESULTS:' % (search_string, len(search_results))\n",
      "print\n",
      "print search_results[num]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import nltk\n",
      "import re\n",
      "from nltk.corpus import brown\n",
      "sent_tokenizer=nltk.data.load('tokenizers/punkt/english.pickle')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Create function to tokenize.\n",
      "def sent_tokenize(text):\n",
      "    return sent_tokenizer.tokenize(text)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Tokenize each message in data_dict with NLTK tokenizer in dict comprehension:\n",
      "sents = {messageID: sent_tokenize(text) for (messageID, text) in data_dict.items()}"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# This saves the sents dictionary to disk, saving time on load of the iPython notebook.\n",
      "import cPickle as pickle\n",
      "with open('sents.p', 'wb') as fp:\n",
      "    pickle.dump(sents, fp, -1)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# This loads the sents dictionary from disk.\n",
      "with open('sents.p', 'rb') as fp:\n",
      "    sents = pickle.load(fp)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Determine number of sentences:\n",
      "count = 0\n",
      "for sentences in sents.values():\n",
      "    count += len(sentences)\n",
      "print \"# of sentences:\", count"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# To deal with hard returns:\n",
      "for messageID, value in sents.items():\n",
      "    sents[messageID] = [sent.replace('\\n', ' ') for sent in value] # to deal with hard wordwrapping"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "###Use the below to take a look at sentences of random messages:###"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "num = random.randint(0, len(data_dict))\n",
      "for messageID, sentences in sents.items()[num:num+1]:\n",
      "    print 'NEW MESSAGE: %s' % messageID\n",
      "    for sent in sentences:\n",
      "        print 'SENTENCE:', sent\n",
      "        print"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Regular Expression pattern for tokenizing words.\n",
      "pattern = r'''(?x)     # set flag to allow verbose regexps\n",
      "     (http://[\\w\\.,-_/]+) # Full URL\n",
      "    |(www.[\\w\\.,-_/]+)    # partial URL\n",
      "    |(^/[\\w\\.,-_]+)      # broken URL\n",
      "    |([A-Z]\\.)+          # abbreviations, e.g. U.S.A.\n",
      "    | \\w+([-']+\\w+)*     # words with optional internal hyphens\n",
      "    |\\$?\\d+(\\.\\d+)?%?    # currency and percentages, e.g. $12.40, 82%\n",
      "    | \\.\\.\\.             # ellipsis\n",
      "    | [][.,;\"'?():-_`]   # these are separate tokens\n",
      " '''"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "###Use the below to take a look at tokens in sentences of random messages:###"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "num = random.randint(0, len(data_dict))\n",
      "\n",
      "for messageID, value in sents.items()[num:num+1]:\n",
      "    print 'NEW MESSAGE: %s' % messageID\n",
      "    for sent in value:\n",
      "        print 'SENTENCE:', sent\n",
      "        print nltk.regexp_tokenize(sent,pattern)\n",
      "        print"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### POS Tagging"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#Function to build POS tagger\n",
      "def build_backoff_tagger(train_sents):\n",
      "    t0 = nltk.DefaultTagger('NN')\n",
      "    t1 = nltk.UnigramTagger(train_sents, backoff=t0)\n",
      "    t2 = nltk.BigramTagger(train_sents, backoff=t1)\n",
      "    t3 = nltk.TrigramTagger(train_sents, backoff=t2)\n",
      "    return t3\n",
      "\n",
      "# Train a tagger on all of the Brown corpus\n",
      "training_data = brown.tagged_sents()\n",
      "ngram_tagger = build_backoff_tagger(training_data)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Tokenize and Tag POS Words in Sentences"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "mails = {}\n",
      "tagged_mails = {}\n",
      "for key, value in sents.items():\n",
      "    sents_words = []\n",
      "    tagged_sents = []\n",
      "    for sent in value:\n",
      "        words = nltk.regexp_tokenize(sent,pattern)\n",
      "        sents_words.append(words)\n",
      "        tagged_words = ngram_tagger.tag(words)\n",
      "        tagged_sents.append(tagged_words)\n",
      "    \n",
      "    mails[key] = sents_words\n",
      "    tagged_mails[key] =  tagged_sents"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# This saves the mails and tagged_mails dictionaries to disk, saving time on load of the iPython notebook.\n",
      "with open('mails.p', 'wb') as fp:\n",
      "    pickle.dump(mails, fp, -1)\n",
      "with open('tagged_mails.p', 'wb') as fp:\n",
      "    pickle.dump(tagged_mails, fp, -1)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# This loads the mails and tagged_mails dictionaries from disk.\n",
      "import cPickle as pickle\n",
      "with open('mails.p', 'rb') as fp:\n",
      "    mails = pickle.load(fp)\n",
      "with open('tagged_mails.p', 'rb') as fp:\n",
      "    tagged_mails = pickle.load(fp)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "###Test: Verify mails and tagged_mails messages function properly."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "message = tagged_mails['<loom.20081221T153948-465@post.gmane.org>']\n",
      "print message"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print tagged_mails['<AANLkTi=e78R-YRtvb2Ta-s=p24=+nkRK2p_eZ-84GQ0W@mail.gmail.com>']"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "###Explicitly Randomize tagged_mails###"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import random\n",
      "from __future__ import division"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "unordered=tagged_mails.items() # List of unordered tuples\n",
      "random.shuffle(unordered)\n",
      "third = int(len(unordered)/3)\n",
      "training = {messageID: message for (messageID,message) in unordered[:third*2]}\n",
      "dev_test = {messageID: message for (messageID,message) in unordered[third*2:]}"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# This saves the training and dev_test dictionaries to disk, saving time on load of the iPython notebook.\n",
      "with open('training.p', 'wb') as fp:\n",
      "    pickle.dump(training, fp, -1)\n",
      "with open('dev_test.p', 'wb') as fp:\n",
      "    pickle.dump(dev_test, fp, -1)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}