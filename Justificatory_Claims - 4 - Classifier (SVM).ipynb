{
 "metadata": {
  "name": "",
  "signature": "sha256:f6517dfeee03baf4c5a1341c604afd60a250e2e41f241ff18522d9d06673b648"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#Training Extraction#"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import nltk\n",
      "import random\n",
      "import math\n",
      "import numpy\n",
      "from sklearn import svm, cross_validation"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 16
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##Read hand-coded data from json file"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "with open('./justification_tags4.json', 'rb') as file:\n",
      "    data = eval(file.read())"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "hand_coded_data = list()\n",
      "for x in data:\n",
      "    lst = list()\n",
      "    lst.append(([(z[0], z[1]) for z in x['sent']],[(z[0], z[1]) for z in x['prev']]))\n",
      "    lst.append(x['justification'])\n",
      "    hand_coded_data.append(lst)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "types = list()\n",
      "for x in hand_coded_data:\n",
      "    if x[1] not in types:\n",
      "        types.append(x[1])\n",
      "categories = dict(enumerate(types, start=0))\n",
      "print categories"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "{0: 'EXPERIENCE', 1: 'NO JUSTIFICATION', 2: 'OTHER', 3: 'GENERALIZATION', 4: 'ANALOGY', 5: 'AUTHORITY'}\n"
       ]
      }
     ],
     "prompt_number": 5
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "hand_coded_data = [(tup, types.index(typ)) for (tup, typ) in hand_coded_data]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 6
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print len(hand_coded_data)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "403\n"
       ]
      }
     ],
     "prompt_number": 17
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "random.shuffle(hand_coded_data)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 23
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#Feature Extraction#"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Extraction Functions"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Number of words of two text-spans\n",
      "def number_of_words_feature(features, data): \n",
      "    features.append( int(len(data[0]) + len(data[1]) / 10 ) ) \n",
      "    return features\n",
      "\n",
      "# Number of unique words of two text-spans\n",
      "def number_of_unique_word(features, data):\n",
      "    tuples = data[0] + data[1]\n",
      "    tokens = [tup[0] for tup in tuples]\n",
      "\n",
      "    features.append( int(len(set(tokens)) / 5) )\n",
      "    return features\n",
      "\n",
      "# This extracts tokens local to the cue signals.\n",
      "def local_tokens(features, data):\n",
      "    cue_phrases = ['so', 'because', 'since', 'per', 'thus', 'therefore', 'consequently']\n",
      "    tuples = data[0] + data[1]\n",
      "    tokens = [tup[0] for tup in tuples]\n",
      "    \n",
      "    vector = [0, 0]\n",
      "    for n, token in enumerate(tokens):\n",
      "        if token in cue_phrases:\n",
      "            if len(tokens) > n+1:\n",
      "                vector[0] = 1\n",
      "            if len(tokens) > n+2:\n",
      "                vector[1] = 1\n",
      "            break\n",
      "    features.extend(vector)\n",
      "    return features\n",
      "\n",
      "# This simply extracts the first cue_signal.\n",
      "def first_cue_signal(features, data):\n",
      "    cue_phrases = ['so', 'because', 'since', 'per', 'thus', 'therefore', 'consequently']\n",
      "    tuples = data[0] + data[1]\n",
      "    tokens = [tup[0] for tup in tuples]\n",
      "    \n",
      "    vector = [0, 0, 0, 0, 0, 0, 0, 0]\n",
      "    for n, token in enumerate(tokens):\n",
      "        if token in cue_phrases:\n",
      "            vector[cue_phrases.index(token)] = 1\n",
      "            break\n",
      "\n",
      "    features.extend(vector)\n",
      "    return features\n",
      "\n",
      "# This simply IDs cue signals:\n",
      "def cue_signals_second(features, data):\n",
      "    cue_phrases = ['so', 'because', 'since', 'per', 'thus', 'therefore', 'consequently']\n",
      "    tuples = data[1]\n",
      "    tokens = [tup[0] for tup in tuples]\n",
      "    vector = [0, 0, 0, 0, 0, 0, 0]\n",
      "    for n, token in enumerate(tokens):\n",
      "        if token in cue_phrases:\n",
      "            vector[cue_phrases.index(token)] = 1\n",
      "    features.extend(vector)\n",
      "    return features\n",
      "\n",
      "# This simply extracts the last cue_signal.\n",
      "def last_cue_signal(features, data):\n",
      "    cue_phrases = ['so', 'because', 'since', 'per', 'thus', 'therefore', 'consequently']\n",
      "    tuples = data[0] + data[1]\n",
      "    tokens = [tup[0] for tup in tuples]\n",
      "    \n",
      "    vector = [0, 0, 0, 0, 0, 0, 0, 0]\n",
      "    for n, token in enumerate(tokens[::-1]):\n",
      "        if token in cue_phrases:\n",
      "            vector[cue_phrases.index(token)] = 1\n",
      "            break\n",
      "\n",
      "    features.extend(vector)\n",
      "    return features\n",
      "\n",
      "# This simply extracts the POS of the first cue_signal.\n",
      "def first_pos_cue_signal(features, data):\n",
      "    cue_phrases = ['so', 'because', 'since', 'per', 'thus', 'therefore', 'consequently']\n",
      "    tuples = data[0] + data[1]\n",
      "    vector = [0]\n",
      "    for n, tup in enumerate(tuples):\n",
      "        if tup[0] in cue_phrases:\n",
      "            vector[0] = 1\n",
      "            break\n",
      "    features.extend(vector)\n",
      "    return features\n",
      "\n",
      "\n",
      "# This simply extracts the index of the second textspan cue_signal.\n",
      "def cue_signal_index(features, data):\n",
      "    cue_phrases = ['so', 'because', 'since', 'per', 'thus', 'therefore', 'consequently']\n",
      "    tuples = data[1]\n",
      "    tokens = [tup[0] for tup in tuples]\n",
      "    \n",
      "    vector = [0]\n",
      "    for n, token in enumerate(tokens):\n",
      "        if token in cue_phrases:\n",
      "            vector[0] = 1\n",
      "            break\n",
      "    features.extend(vector)\n",
      "    return features\n",
      "\n",
      "# This distinguishes only this with their second textspan longer than 90 tokens.\n",
      "def leng(features, data):\n",
      "    tuples = data[1]\n",
      "    tokens = [tup[0] for tup in tuples]\n",
      "    vector = []\n",
      "    if len(tokens) > 90:\n",
      "        vector.append(1)\n",
      "    else:\n",
      "        vector.append(0)\n",
      "    features.extend(vector)\n",
      "    return features\n",
      "\n",
      "# If the second text-span contains pronouns\n",
      "def pronouns(features, data):\n",
      "    tuples = data[0] + data[1]\n",
      "    \n",
      "    vector = []\n",
      "    vector.append(1) if len([tup[1] for tup in tuples if tup[1].startswith('PPSS')]) != 0 else vector.append(0)\n",
      "    features.extend(vector)\n",
      "    return features\n",
      "\n",
      "# If the first text-span contains verbs\n",
      "def verbs(features, data):\n",
      "    tuples = data[0] + data[1]\n",
      "    vector = []\n",
      "    vector.append(1) if len([tup[1] for tup in tuples if tup[1].startswith('VBZ')]) != 0 else vector.append(0)\n",
      "    features.extend(vector)\n",
      "    \n",
      "    return features"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 9
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### TF-IDF features\n",
      "This part is orginally from Daniel's Kaggle assignment notebook and adapted by our team)"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Category words list to 6 categories(types of justification) \n",
      "def words_pos_per_category(data, flag = False):\n",
      "    category_words = [[],[],[],[],[],[]]\n",
      "    category_pos = [[],[],[],[],[],[]]\n",
      "    category_tuple = [[],[],[],[],[],[]]\n",
      "    \n",
      "    for tup, tag in data:\n",
      "        try:\n",
      "            tuples = tup[0] + tup[1]\n",
      "            tokens_word = [tup[0] for tup in tuples]\n",
      "            tokens_pos = [tup[1] for tup in tuples]\n",
      "            \n",
      "            # Bigrams\n",
      "            if flag:\n",
      "                tokens_word = nltk.bigrams(tokens_word)\n",
      "                tokens_pos = nltk.bigrams(tokens_pos)\n",
      "                tuples = nltk.bigrams(tuples)\n",
      "            \n",
      "            category_words[tag].extend(tokens_word)\n",
      "            category_pos[tag].extend(tokens_pos)\n",
      "            category_tuple[tag].extend(tuples)\n",
      "        except:\n",
      "            pass\n",
      "    return category_words, category_pos, category_tuple\n",
      "\n",
      "# Get the FreqDist of words list belonged to different types of justification\n",
      "def FreqDists_per_category(category_term):\n",
      "    categoryFreqDists = []\n",
      "    for category in category_term:\n",
      "        categoryFreqDists.append(nltk.FreqDist([w for w in category]))\n",
      "    return categoryFreqDists\n",
      "\n",
      "# Calculate TF-IDF score\n",
      "def get_tfidf_score_list(categories, categoryFreqDists):\n",
      "    tfidf = []\n",
      "    for n, each in enumerate(categories):\n",
      "        for key in categoryFreqDists[n].keys():\n",
      "            tf = categoryFreqDists[n][key]/len(categoryFreqDists[n].keys())\n",
      "            D = 0\n",
      "            for n2, each2 in enumerate(categories):\n",
      "                if key in categoryFreqDists[n2].keys():\n",
      "                    D += 1\n",
      "            IDF = math.log(len(categories)/D)\n",
      "            tf_IDF = math.log((1+tf))*IDF\n",
      "            tfidf.append((n+1, key, tf_IDF))\n",
      "    return tfidf\n",
      "\n",
      "# Get sorted term list with TF-IDF score\n",
      "def get_general_tfidf(categories, categoryWordFreqDists, categoryPOSFreqDists, categoryTupleFreqDists):\n",
      "    '''tf.idf  equation from week6b_hearst_anlp2014.pdf, code written by Daniel'''\n",
      "    tfidf_word = get_tfidf_score_list(categories, categoryWordFreqDists)\n",
      "    tfidf_pos = get_tfidf_score_list(categories, categoryPOSFreqDists)\n",
      "    tfidf_tuple = get_tfidf_score_list(categories, categoryTupleFreqDists)\n",
      "\n",
      "    sorted_general_word_tfidf = sorted(tfidf_word,key=lambda x: x[2], reverse = True) # Sort from largest to smallest.\n",
      "    sorted_general_pos_tfidf = sorted(tfidf_pos,key=lambda x: x[2], reverse = True) # Sort from largest to smallest.\n",
      "    sorted_general_tuple_tfidf = sorted(tfidf_tuple,key=lambda x: x[2], reverse = True) # Sort from largest to smallest.\n",
      "    \n",
      "    return sorted_general_word_tfidf, sorted_general_pos_tfidf, sorted_general_tuple_tfidf\n",
      "\n",
      "# Get the terms with higher TF-IDF score \n",
      "# size: the number of terms\n",
      "def top_general_tfidf(sorted_general_tfidf, size):\n",
      "    top = []\n",
      "    for tag, token, freq in sorted_general_tfidf[:size]: top.append(token)\n",
      "    return set(top)\n",
      "\n",
      "# The whole process of TF-IDF score data \n",
      "def prep_bulk_features(train, flag = False):\n",
      "    category_words, category_pos, category_tuple = words_pos_per_category(train, flag)\n",
      "    categoryWordFreqDists = FreqDists_per_category(category_words)\n",
      "    categoryPOSFreqDists = FreqDists_per_category(category_pos)\n",
      "    categoryTupleFreqDists = FreqDists_per_category(category_tuple)\n",
      "\n",
      "    word_tfidf, pos_tfidf, tuple_tfidf = \\\n",
      "        get_general_tfidf(categories, categoryWordFreqDists, categoryPOSFreqDists, categoryTupleFreqDists)\n",
      "\n",
      "    return word_tfidf, pos_tfidf, tuple_tfidf\n",
      "\n",
      "def tfidf_feature(features, data, word_tfidf, pos_tfidf, tuple_tfidf, option, flag = False):\n",
      "    \"\"\"\"This pulls features from the records, line by line, and compares them with three \n",
      "    preprocessed sets of items created through identifying tf.idf for the items.\"\"\"\n",
      "\n",
      "    try:\n",
      "        tuples = data[0] + data[1]\n",
      "        if option == 'pos':\n",
      "            tokens = [tup[1] for tup in tuples]\n",
      "            tfidf = pos_tfidf\n",
      "            threshold = 4        \n",
      "        elif option == 'word' : \n",
      "            tokens = [tup[0] for tup in tuples]\n",
      "            tfidf = word_tfidf\n",
      "            threshold = 120\n",
      "        else:\n",
      "            tokens = tuples\n",
      "            tfidf = tuple_tfidf\n",
      "            threshold = 200  \n",
      "\n",
      "        if flag:\n",
      "            tokens = nltk.bigrams(tokens)\n",
      "\n",
      "        prepare_features = []\n",
      "        vector = []\n",
      "        for n, each in enumerate(tokens):            \n",
      "            for term in top_general_tfidf(tfidf, threshold):\n",
      "                if term == each:\n",
      "                    prepare_features.append(each)\n",
      "\n",
      "        for term in top_general_tfidf(tfidf, threshold):\n",
      "            if term in prepare_features:\n",
      "                vector.append(1)\n",
      "            else:\n",
      "                vector.append(0) \n",
      "\n",
      "        features.extend(vector)\n",
      "    except:\n",
      "        print 'exception'\n",
      "\n",
      "    return features\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 43
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Full features\n",
      "1. TF-IDF of pos tag\n",
      "2. TF-IDF of tuple:(word, tag)\n",
      "3. Local tokens\n",
      "4. First cue signal\n",
      "5. Second cue signal\n",
      "6. Last cue signal\n",
      "7. Cue signal index\n",
      "8. Leng : if the length of second text-span is longer than 90"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def full_feature_extraction_svm(data, word_tfidf, pos_tfidf, tuple_tfidf):\n",
      "    features = []\n",
      "    features = tfidf_feature(features, data, word_tfidf, pos_tfidf, tuple_tfidf, 'pos')\n",
      "    features = tfidf_feature(features, data, word_tfidf, pos_tfidf, tuple_tfidf, 'tuple')\n",
      "    \n",
      "    features = local_tokens(features, data)\n",
      "    \n",
      "    features = first_cue_signal(features, data)\n",
      "    features = cue_signals_second(features, data)\n",
      "    features = last_cue_signal(features, data)\n",
      "    features = cue_signal_index(features, data)\n",
      "    \n",
      "    features = leng(features, data)\n",
      "    return features"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 41
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## SVM Classifier"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Show the results which are not classified correctly"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def show_guess_data_svm(classifier, test_data, test_data_tag, number):  \n",
      "    # This is code from the NLTK chapter\n",
      "    errors = []\n",
      "    for idx, tup in enumerate(test_data):\n",
      "        guess = classifier.predict(tup)\n",
      "        if guess[0] != test_data_tag[idx]:\n",
      "            errors.append( (test_data_tag[idx], guess, tup, idx) )\n",
      "            \n",
      "    print 'Error guess:%d, Correct guess:%d'% (len(errors), len(test_data) - len(errors))\n",
      "    \n",
      "    for (tag, guess, tup, idx) in errors[:number]:\n",
      "        print 'correct=%-8s guess=%-8s' % (tag, guess[0])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 19
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Cross validation on 10 folds using SVM classifier"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def crossValidation_svm(data, number_folds):  \n",
      "    train_test_cv = cross_validation.KFold(len(data), n_folds=number_folds, shuffle=False, random_state=None)   \n",
      "    \n",
      "    accuracy_list = []\n",
      "    \n",
      "    for idxcv, (train_cv, test_cv) in enumerate(train_test_cv):\n",
      "        \n",
      "        train_data = [(tup, tag) for idx, (tup, tag) in enumerate(data) if idx in train_cv]\n",
      "        test_data = [(tup, tag) for idx, (tup, tag) in enumerate(data) if idx in test_cv]\n",
      "        \n",
      "        word_tfidf, pos_tfidf, tuple_tfidf = prep_bulk_features(train_data)\n",
      "        train_features = [(full_feature_extraction_svm(tup, word_tfidf, pos_tfidf, tuple_tfidf), tag) for (tup, tag) in train_data]\n",
      "        test_features = [(full_feature_extraction_svm(tup, word_tfidf, pos_tfidf, tuple_tfidf), tag) for (tup, tag) in test_data]    \n",
      " \n",
      "        X_train = [features for (features, tag) in train_features]\n",
      "        y_train = [tag for (features, tag) in train_features]\n",
      "        X_test = [features for (features, tag) in test_features]\n",
      "        y_test = [tag for (features, tag) in test_features]\n",
      "   \n",
      "        clf = svm.LinearSVC()\n",
      "        try:\n",
      "            clf.fit(X_train, y_train)\n",
      "            clf.score(X_test, y_test)\n",
      "        except:\n",
      "            print 'exception'\n",
      "        \n",
      "        accuracy_list.append(clf.score(X_test, y_test))\n",
      "        \n",
      "        show_guess_data_svm(clf, X_test, y_test, 5)\n",
      "        print\n",
      "        \n",
      "    print \"Accuracy for each fold\"\n",
      "    for accuracy in accuracy_list:\n",
      "        print accuracy\n",
      "        \n",
      "    print \"Average Accuracy:%f\" % numpy.mean(accuracy_list)\n",
      "\n",
      "crossValidation_svm(hand_coded_data, 10)\n",
      "# {0: 'EXPERIENCE', 1: 'NO JUSTIFICATION', 2: 'OTHER', 3: 'GENERALIZATION', 4: 'ANALOGY', 5: 'AUTHORITY'}"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Error guess:35, Correct guess:6\n",
        "correct=1        guess=5       \n",
        "correct=3        guess=5       \n",
        "correct=5        guess=3       \n",
        "correct=3        guess=2       \n",
        "correct=4        guess=1       \n",
        "\n",
        "Error guess:28, Correct guess:13"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "correct=3        guess=4       \n",
        "correct=2        guess=4       \n",
        "correct=4        guess=2       \n",
        "correct=1        guess=3       \n",
        "correct=3        guess=4       \n",
        "\n",
        "Error guess:24, Correct guess:17"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "correct=3        guess=4       \n",
        "correct=1        guess=4       \n",
        "correct=4        guess=1       \n",
        "correct=3        guess=5       \n",
        "correct=0        guess=4       \n",
        "\n",
        "Error guess:31, Correct guess:9"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "correct=1        guess=4       \n",
        "correct=4        guess=3       \n",
        "correct=2        guess=5       \n",
        "correct=2        guess=3       \n",
        "correct=4        guess=5       \n",
        "\n",
        "Error guess:22, Correct guess:18"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "correct=4        guess=1       \n",
        "correct=4        guess=3       \n",
        "correct=4        guess=5       \n",
        "correct=4        guess=1       \n",
        "correct=5        guess=2       \n",
        "\n",
        "Error guess:28, Correct guess:12"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "correct=0        guess=4       \n",
        "correct=3        guess=4       \n",
        "correct=2        guess=1       \n",
        "correct=3        guess=1       \n",
        "correct=1        guess=2       \n",
        "\n",
        "Error guess:28, Correct guess:12"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "correct=1        guess=3       \n",
        "correct=3        guess=4       \n",
        "correct=4        guess=3       \n",
        "correct=0        guess=1       \n",
        "correct=5        guess=3       \n",
        "\n",
        "Error guess:30, Correct guess:10"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "correct=1        guess=4       \n",
        "correct=4        guess=3       \n",
        "correct=3        guess=1       \n",
        "correct=1        guess=5       \n",
        "correct=2        guess=4       \n",
        "\n",
        "Error guess:24, Correct guess:16"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "correct=3        guess=2       \n",
        "correct=2        guess=4       \n",
        "correct=5        guess=2       \n",
        "correct=5        guess=4       \n",
        "correct=5        guess=2       \n",
        "\n",
        "Error guess:28, Correct guess:12"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "correct=2        guess=4       \n",
        "correct=3        guess=5       \n",
        "correct=5        guess=1       \n",
        "correct=1        guess=4       \n",
        "correct=5        guess=4       \n",
        "\n",
        "Accuracy for each fold\n",
        "0.146341463415\n",
        "0.317073170732\n",
        "0.414634146341\n",
        "0.225\n",
        "0.45\n",
        "0.3\n",
        "0.3\n",
        "0.25\n",
        "0.4\n",
        "0.3\n",
        "Average Accuracy:0.310305\n"
       ]
      }
     ],
     "prompt_number": 42
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}