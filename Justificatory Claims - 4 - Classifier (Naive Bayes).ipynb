{
 "metadata": {
  "name": "",
  "signature": "sha256:37ea463167e07c9bf8af4312b743f567fb3d174d4b6d9a9c7754307afb0edf22"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#Training Extraction#"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Import statments:\n",
      "import csv\n",
      "import itertools\n",
      "import nltk\n",
      "import random\n",
      "from __future__ import division\n",
      "import time\n",
      "import cPickle as pickle \n",
      "import json\n",
      "import math\n",
      "import numpy\n",
      "from sklearn import cross_validation\n",
      "from collections import Counter"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "with open('../version2/justification_tags4.json', 'rb') as file:\n",
      "    data = eval(file.read())"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "hand_coded_data = list()\n",
      "for x in data:\n",
      "    lst = list()\n",
      "    lst.append(([(z[0], z[1]) for z in x['sent']],[(z[0], z[1]) for z in x['prev']]))\n",
      "    lst.append(x['justification'])\n",
      "    hand_coded_data.append(lst)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "types = list()\n",
      "for x in hand_coded_data:\n",
      "    if x[1] not in types:\n",
      "        types.append(x[1])\n",
      "categories = dict(enumerate(types, start=0))\n",
      "print categories"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "{0: 'EXPERIENCE', 1: 'NO JUSTIFICATION', 2: 'OTHER', 3: 'GENERALIZATION', 4: 'ANALOGY', 5: 'AUTHORITY'}\n"
       ]
      }
     ],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "hand_coded_data = [(tup, types.index(typ)) for (tup, typ) in hand_coded_data]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 5
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#Feature Extraction#"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Extraction Functions"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def number_of_words_feature(features, data): \n",
      "    features['number_of_words'] = len(data[0]) + len(data[1])\n",
      "    #blah = (data[0] + data[1])\n",
      "    #features['number_of_words'] = len(set(blah))   \n",
      "    return features\n",
      "\n",
      "def has_CD_tag_feature(features, data):\n",
      "    tuples = data[0] + data[1]\n",
      "#     count = 0\n",
      "    for word, POS in tuples:\n",
      "#         if POS == 'CD':\n",
      "#             count = count + 1\n",
      "        if POS in features.keys():\n",
      "            features[POS] += 1\n",
      "        else:\n",
      "            features[POS] = 1\n",
      "\n",
      "#     features['CD'] = count        \n",
      "    return features\n",
      "\n",
      "def number_of_unique_word(features, data):\n",
      "    tuples = data[0] + data[1]\n",
      "    tokens = [tup[0] for tup in tuples]\n",
      "\n",
      "    features['number_of_unique_word'] = len(set(tokens))   \n",
      "    return features\n",
      "\n",
      "# This extracts tokens local to the cue signals.\n",
      "def local_tokens(features, data):\n",
      "    cue_phrases = ['so', 'because', 'since', 'per', 'thus', 'therefore', 'consequently']\n",
      "    tuples = data[0] + data[1]\n",
      "    tokens = [tup[0] for tup in tuples]\n",
      "    \n",
      "    for n, token in enumerate(tokens):\n",
      "        if token in cue_phrases:\n",
      "            if len(tokens) > n+1:\n",
      "                features['local1'+tokens[n+1]] = True\n",
      "            if len(tokens) > n+2:\n",
      "                features['local2'+tokens[n+2]] = True\n",
      "            break\n",
      "    return features\n",
      "\n",
      "# This simply extracts the first cue_signal.\n",
      "def first_cue_signal(features, data):\n",
      "    cue_phrases = ['so', 'because', 'since', 'per', 'thus', 'therefore', 'consequently']\n",
      "    tuples = data[0] + data[1]\n",
      "    tokens = [tup[0] for tup in tuples]\n",
      "    for n, token in enumerate(tokens):\n",
      "        if token in cue_phrases:\n",
      "            features['first_cue_signal'] = token\n",
      "            break\n",
      "    return features\n",
      "\n",
      "# This simply IDs cue signals:\n",
      "def cue_signals_second(features, data):\n",
      "    cue_phrases = ['so', 'because', 'since', 'per', 'thus', 'therefore', 'consequently']\n",
      "    tuples = data[1]\n",
      "    tokens = [tup[0] for tup in tuples]\n",
      "    for n, token in enumerate(tokens):\n",
      "        if token in cue_phrases:\n",
      "            features[token] = True\n",
      "    return features\n",
      "\n",
      "# This simply extracts the last cue_signal.\n",
      "def last_cue_signal(features, data):\n",
      "    cue_phrases = ['so', 'because', 'since', 'per', 'thus', 'therefore', 'consequently']\n",
      "    tuples = data[0] + data[1]\n",
      "    tokens = [tup[0] for tup in tuples]\n",
      "    for n, token in enumerate(tokens[::-1]):\n",
      "        if token in cue_phrases:\n",
      "            features['last_cue_signal'] = token\n",
      "            break\n",
      "    return features\n",
      "\n",
      "# This simply extracts the POS of the second data cue_signal.\n",
      "def pos_cue_signal(features, data):\n",
      "    cue_phrases = ['so', 'because', 'since', 'per', 'thus', 'therefore', 'consequently']\n",
      "    tuples = data[1]\n",
      "    for n, tup in enumerate(tuples):\n",
      "        if tup[0] in cue_phrases:\n",
      "            features['pos_cue_signal'] = tup[1]\n",
      "            break\n",
      "    return features\n",
      "\n",
      "# This simply extracts the POS of the first cue_signal.\n",
      "def first_pos_cue_signal(features, data):\n",
      "    cue_phrases = ['so', 'because', 'since', 'per', 'thus', 'therefore', 'consequently']\n",
      "    tuples = data[0] + data[1]\n",
      "    for n, tup in enumerate(tuples):\n",
      "        if tup[0] in cue_phrases:\n",
      "            features['first_pos_cue_signal'] = tup[1]\n",
      "            break\n",
      "    return features\n",
      "\n",
      "\n",
      "# This simply extracts the index of the second textspan cue_signal.\n",
      "def cue_signal_index(features, data):\n",
      "    cue_phrases = ['so', 'because', 'since', 'per', 'thus', 'therefore', 'consequently']\n",
      "    tuples = data[1]\n",
      "    tokens = [tup[0] for tup in tuples]\n",
      "    for n, token in enumerate(tokens):\n",
      "        if token in cue_phrases:\n",
      "            features['cue_signal_index'] = n\n",
      "            break\n",
      "    return features\n",
      "\n",
      "# This distinguishes only this with their second textspan longer than 90 tokens.\n",
      "def leng(features, data):\n",
      "    tuples = data[1]\n",
      "    tokens = [tup[0] for tup in tuples]\n",
      "    if len(tokens) > 90:\n",
      "        features['leng'] = 2\n",
      "    else:\n",
      "        features['leng'] = 1\n",
      "    return features\n",
      "\n",
      "def pronouns(features, data):\n",
      "    tuples = data[0] + data[1]\n",
      "    features['pronouns'] = True if len([tup[1] for tup in tuples if tup[1].startswith('PPSS')]) != 0 else False\n",
      "\n",
      "    return features\n",
      "\n",
      "def verbs(features, data):\n",
      "    tuples = data[0] + data[1]\n",
      "    features['verbs'] = True if len([tup[1] for tup in tuples if tup[1].startswith('VBZ')]) != 0 else False\n",
      "    \n",
      "    return features\n",
      "    \n",
      "def most_common_pos(features, data):\n",
      "    tuples = data[0] + data[1]\n",
      "    features['most_common'] = Counter([tup[1] for tup in tuples]).most_common(1)[0][0]\n",
      "    return features"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 8
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# TF-IDF\n",
      "def words_pos_per_category(data):\n",
      "    category_words = [[],[],[],[],[],[],[]]\n",
      "    category_pos = [[],[],[],[],[],[],[]]\n",
      "    category_tuple = [[],[],[],[],[],[],[]]\n",
      "    \n",
      "    for tup, tag in data:\n",
      "        try:\n",
      "            tuples = tup[0] + tup[1]\n",
      "            tokens_word = [tup[0] for tup in tuples]\n",
      "            tokens_pos = [tup[1] for tup in tuples]\n",
      "            category_words[tag].extend(tokens_word)\n",
      "            category_pos[tag].extend(tokens_pos)\n",
      "            category_tuple[tag].extend(tuples)\n",
      "        except:\n",
      "            pass\n",
      "    \n",
      "#     print num\n",
      "#     for each in category_words:\n",
      "#         each = [item for sublist in each for item in sublist] # To flatten the nested list of lists.\n",
      "        \n",
      "#     return category_words\n",
      "    return category_words, category_pos, category_tuple\n",
      "\n",
      "def FreqDists_per_category(category_term):\n",
      "    categoryFreqDists = []\n",
      "    for category in category_term:\n",
      "        categoryFreqDists.append(nltk.FreqDist([w for w in category]))\n",
      "#         if isinstance(category[0], str):\n",
      "#             categoryFreqDists.append(nltk.FreqDist([w for w in category]))\n",
      "#         else:\n",
      "#             categoryFreqDists.append(nltk.FreqDist([w for w in category]))\n",
      "    return categoryFreqDists\n",
      "\n",
      "def get_general_tfidf(categories, categoryWordFreqDists, categoryPOSFreqDists, categoryTupleFreqDists):\n",
      "    '''tf.idf  equation from week6b_hearst_anlp2014.pdf, code written by Daniel'''\n",
      "    tfidf_word = []\n",
      "    tfidf_pos = []\n",
      "    tfidf_tuple = []\n",
      "    for n, each in enumerate(categories):\n",
      "        for key in categoryWordFreqDists[n].keys():#[:275]: # This was previously set to [:100]\n",
      "                tf = categoryWordFreqDists[n][key]/len(categoryWordFreqDists[n].keys())\n",
      "                D = 0\n",
      "                for n2, each2 in enumerate(categories):\n",
      "                    if key in categoryWordFreqDists[n2].keys():\n",
      "                        D += 1\n",
      "                IDF = math.log(len(categories)/D)\n",
      "                tf_IDF = math.log((1+tf))*IDF\n",
      "                tfidf_word.append((n+1, key, tf_IDF))\n",
      "#     sorted_general_tfidf = sorted(tfidf,key=lambda x: x[2], reverse = True) # Sort from largest to smallest.   \n",
      "#     return sorted_general_tfidf\n",
      "\n",
      "        for key in categoryPOSFreqDists[n].keys():#[:275]: # This was previously set to [:100]\n",
      "                tf = categoryPOSFreqDists[n][key]/len(categoryPOSFreqDists[n].keys())\n",
      "                D = 0\n",
      "                for n2, each2 in enumerate(categories):\n",
      "                    if key in categoryPOSFreqDists[n2].keys():\n",
      "                        D += 1\n",
      "                IDF = math.log(len(categories)/D)\n",
      "                tf_IDF = math.log((1+tf))*IDF\n",
      "                tfidf_pos.append((n+1, key, tf_IDF))\n",
      "                \n",
      "        for key in categoryTupleFreqDists[n].keys():#[:275]: # This was previously set to [:100]\n",
      "                tf = categoryTupleFreqDists[n][key]/len(categoryTupleFreqDists[n].keys())\n",
      "                D = 0\n",
      "                for n2, each2 in enumerate(categories):\n",
      "                    if key in categoryTupleFreqDists[n2].keys():\n",
      "                        D += 1\n",
      "                IDF = math.log(len(categories)/D)\n",
      "                tf_IDF = math.log((1+tf))*IDF\n",
      "                tfidf_tuple.append((n+1, key, tf_IDF))\n",
      "    \n",
      "    sorted_general_word_tfidf = sorted(tfidf_word,key=lambda x: x[2], reverse = True) # Sort from largest to smallest.\n",
      "    sorted_general_pos_tfidf = sorted(tfidf_pos,key=lambda x: x[2], reverse = True) # Sort from largest to smallest.\n",
      "    sorted_general_tuple_tfidf = sorted(tfidf_tuple,key=lambda x: x[2], reverse = True) # Sort from largest to smallest.\n",
      "    \n",
      "    return sorted_general_word_tfidf, sorted_general_pos_tfidf, sorted_general_tuple_tfidf\n",
      "\n",
      "def top_general_tfidf(sorted_general_tfidf, size):\n",
      "    top = []\n",
      "    for tag, token, freq in sorted_general_tfidf[:size]: top.append(token)\n",
      "    return set(top)\n",
      "\n",
      "def prep_bulk_features(train):\n",
      "    category_words, category_pos, category_tuple = words_pos_per_category(train)\n",
      "    categoryWordFreqDists = FreqDists_per_category(category_words)\n",
      "    categoryPOSFreqDists = FreqDists_per_category(category_pos)\n",
      "    categoryTupleFreqDists = FreqDists_per_category(category_tuple)\n",
      "#     sorted_general_tfidf = get_general_tfidf(categories, categoryFreqDists)\n",
      "#     return sorted_general_tfidf\n",
      "\n",
      "    sorted_general_word_tfidf, sorted_general_pos_tfidf, sorted_general_tuple_tfidf = \\\n",
      "        get_general_tfidf(categories, categoryWordFreqDists, categoryPOSFreqDists, categoryTupleFreqDists)\n",
      "        \n",
      "    return sorted_general_word_tfidf, sorted_general_pos_tfidf, sorted_general_tuple_tfidf\n",
      "\n",
      "def tfidf_feature(features, data, word_tfidf, pos_tfidf, tuple_tfidf, option):\n",
      "    \"\"\"\"This pulls features from the records, line by line, and compares them with three \n",
      "    preprocessed sets of items created through identifying tf.idf for the items.\"\"\"\n",
      "\n",
      "    try:\n",
      "        tuples = data[0] + data[1]\n",
      "#         if option == 'word' : \n",
      "#             print 'option2:%s' % option\n",
      "        if option == 'pos':\n",
      "            tokens = [tup[1] for tup in tuples]\n",
      "            tfidf = pos_tfidf\n",
      "            threshold = 4\n",
      "            \n",
      "            \n",
      "        elif option == 'word' : \n",
      "            tokens = [tup[0] for tup in tuples]\n",
      "            tfidf = word_tfidf\n",
      "            threshold = 120\n",
      "        else:\n",
      "            tokens = tuples\n",
      "            tfidf = tuple_tfidf\n",
      "            threshold = 200  \n",
      "        \n",
      "        for n, each in enumerate(tokens):\n",
      "            for term in top_general_tfidf(tfidf, threshold):\n",
      "                if term == each:\n",
      "                    features[term] = 1\n",
      "    except:\n",
      "        print 'exception'\n",
      "    return features\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 9
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def full_feature_extraction(data, word_tfidf, pos_tfidf, tuple_tfidf):\n",
      "    features = {}\n",
      "    \n",
      "#     features = cartesian_product_feature(features, data)\n",
      "#     features = number_of_words_feature(features, data)\n",
      "#     features = tfidf_feature(features, data, word_tfidf, pos_tfidf, tuple_tfidf, 'word')\n",
      "    features = tfidf_feature(features, data, word_tfidf, pos_tfidf, tuple_tfidf, 'pos')\n",
      "    features = tfidf_feature(features, data, word_tfidf, pos_tfidf, tuple_tfidf, 'tuple')\n",
      "#     features = has_CD_tag_feature(features, data)\n",
      "#     features = number_of_unique_word(features, data)\n",
      "\n",
      "    features = local_tokens(features, data)\n",
      "    features = first_cue_signal(features, data)\n",
      "    features = cue_signals_second(features, data)\n",
      "    features = last_cue_signal(features, data)\n",
      "    features = pos_cue_signal(features, data)\n",
      "    features = first_pos_cue_signal(features, data)\n",
      "    features = cue_signal_index(features, data)\n",
      "    features = leng(features, data)\n",
      "    \n",
      "#     features = pronouns(features, data)\n",
      "#     features = verbs(features, data)\n",
      "\n",
      "\n",
      "#     features = most_common_pos(features, data)\n",
      "    \n",
      "    return features\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 10
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# random.shuffle(hand_coded_data)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 11
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Naive Bayes Classifer"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Train classifier"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# def create_train_test_data(feature_function, data, train_data, test_data):\n",
      "#     train_features = [(feature_function(tup), tag) for (tup, tag) in train_data]\n",
      "#     test_features = [(feature_function(tup), tag) for (tup, tag) in test_data]\n",
      "#     return train_features, test_features\n",
      "\n",
      "def crossValidation(data, number_folds):  \n",
      "    train_test_cv = cross_validation.KFold(len(data), n_folds=number_folds, shuffle=False, random_state=None)   \n",
      "    \n",
      "    accuracy_list = []\n",
      "    for train_cv, test_cv in train_test_cv:\n",
      "#         print train_cv\n",
      "#         print \n",
      "#         print test_cv\n",
      "#         print\n",
      "        \n",
      "        train_data = [tup for idx, tup in enumerate(data) if idx in train_cv]\n",
      "        test_data = [tup for idx, tup in enumerate(data) if idx in test_cv]\n",
      "        \n",
      "        word_tfidf, pos_tfidf, tuple_tfidf = prep_bulk_features(train_data)\n",
      "        \n",
      "        train_features = [(full_feature_extraction(tup, word_tfidf, pos_tfidf, tuple_tfidf), tag) for (tup, tag) in train_data]\n",
      "        test_features = [(full_feature_extraction(tup, word_tfidf, pos_tfidf, tuple_tfidf), tag) for (tup, tag) in test_data]\n",
      "    \n",
      "    \n",
      "        \n",
      "#         train_features, test_features = create_train_test_data(full_feature_extraction, data, train_data, test_data) \n",
      "        \n",
      "        # Na\u00efve Bayes Classfier\n",
      "#         print train_features\n",
      "#         print \n",
      "        nb_classifier = nltk.NaiveBayesClassifier.train(train_features)\n",
      "        accuracy_list.append(nltk.classify.accuracy(nb_classifier, test_features))\n",
      "        \n",
      "#         show_guess_data(nb_classifier, test_features, test_data, 1)\n",
      "        nb_classifier.show_most_informative_features(5)\n",
      "        print\n",
      "        \n",
      "    print \"Accuracy for each fold\"\n",
      "    for accuracy in accuracy_list:\n",
      "        print accuracy\n",
      "        \n",
      "    print \"Average Accuracy:%f\" % numpy.mean(accuracy_list)\n",
      "\n",
      "crossValidation(hand_coded_data, 10)\n",
      "# {0: 'EXPERIENCE', 1: 'NO JUSTIFICATION', 2: 'OTHER', 3: 'GENERALIZATION', 4: 'ANALOGY', 5: 'AUTHORITY'}"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Most Informative Features\n",
        "          ('hand', 'NN') = 1                   0 : 1      =      7.9 : 1.0\n",
        "        ('handle', 'VB') = 1                   0 : 4      =      7.8 : 1.0\n",
        "              local1this = True                0 : 4      =      7.8 : 1.0\n",
        "         ('might', 'MD') = 1                   0 : 4      =      7.8 : 1.0\n",
        "                local1of = True                2 : 4      =      7.2 : 1.0\n",
        "\n",
        "Most Informative Features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "        ('always', 'RB') = 1                   0 : 1      =      7.9 : 1.0\n",
        "         last_cue_signal = 'so'                1 : 2      =      6.6 : 1.0\n",
        "                local1of = True                2 : 4      =      6.4 : 1.0\n",
        "                      so = True                1 : 0      =      5.7 : 1.0\n",
        "                 local1I = True                0 : 4      =      5.5 : 1.0\n",
        "\n",
        "Most Informative Features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "                local1of = True                2 : 4      =     10.3 : 1.0\n",
        "        ('handle', 'VB') = 1                   0 : 4      =      8.7 : 1.0\n",
        "        ('always', 'RB') = 1                   0 : 1      =      8.6 : 1.0\n",
        "                local2is = True                3 : 1      =      7.9 : 1.0\n",
        "              local1this = True                0 : 4      =      7.2 : 1.0\n",
        "\n",
        "Most Informative Features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "         last_cue_signal = 'so'                1 : 2      =     10.5 : 1.0\n",
        "        ('handle', 'VB') = 1                   0 : 4      =      8.4 : 1.0\n",
        "        ('always', 'RB') = 1                   0 : 1      =      8.3 : 1.0\n",
        "                      so = True                1 : 0      =      8.1 : 1.0\n",
        "              local1this = True                0 : 4      =      6.9 : 1.0\n",
        "\n",
        "Most Informative Features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "                local1it = True                3 : 1      =      9.3 : 1.0\n",
        "        ('handle', 'VB') = 1                   0 : 4      =      8.2 : 1.0\n",
        "        ('always', 'RB') = 1                   0 : 1      =      7.1 : 1.0\n",
        "                local1of = True                2 : 4      =      7.0 : 1.0\n",
        "                     JJS = 1                   5 : 1      =      6.8 : 1.0\n",
        "\n",
        "Most Informative Features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "        ('handle', 'VB') = 1                   0 : 4      =      8.5 : 1.0\n",
        "                      so = True                1 : 0      =      8.1 : 1.0\n",
        "                local1of = True                2 : 4      =      7.5 : 1.0\n",
        "        ('always', 'RB') = 1                   0 : 1      =      7.4 : 1.0\n",
        "        ('editor', 'NN') = 1                   5 : 4      =      7.1 : 1.0\n",
        "\n",
        "Most Informative Features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "             ('<', 'NN') = 1                   5 : 4      =      9.5 : 1.0\n",
        "        ('handle', 'VB') = 1                   0 : 4      =      8.7 : 1.0\n",
        "        ('always', 'RB') = 1                   0 : 1      =      8.7 : 1.0\n",
        "          ('hand', 'NN') = 1                   0 : 1      =      7.2 : 1.0\n",
        "              local1this = True                0 : 4      =      7.2 : 1.0\n",
        "\n",
        "Most Informative Features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "                local1of = True                2 : 1      =     12.8 : 1.0\n",
        "        ('handle', 'VB') = 1                   0 : 4      =      7.8 : 1.0\n",
        "                local2is = True                3 : 1      =      7.7 : 1.0\n",
        "                local1it = True                3 : 1      =      6.4 : 1.0\n",
        "              local1this = True                0 : 4      =      6.4 : 1.0\n",
        "\n",
        "Most Informative Features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "                local1of = True                2 : 4      =      9.4 : 1.0\n",
        "         ('might', 'MD') = 1                   0 : 4      =      7.1 : 1.0\n",
        "                local1it = True                3 : 1      =      6.7 : 1.0\n",
        "         last_cue_signal = 'so'                1 : 2      =      6.5 : 1.0\n",
        "         ('scope', 'NN') = 1                   3 : 1      =      5.3 : 1.0\n",
        "\n",
        "Most Informative Features"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "         last_cue_signal = 'so'                1 : 2      =     10.7 : 1.0"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "        ('handle', 'VB') = 1                   0 : 4      =      8.4 : 1.0\n",
        "                local1it = True                3 : 1      =      8.2 : 1.0\n",
        "                      so = True                1 : 2      =      7.8 : 1.0\n",
        "                     JJS = 1                   5 : 1      =      7.3 : 1.0\n",
        "\n",
        "Accuracy for each fold\n",
        "0.292682926829\n",
        "0.414634146341\n",
        "0.390243902439\n",
        "0.3\n",
        "0.375\n",
        "0.375\n",
        "0.25\n",
        "0.375\n",
        "0.275\n",
        "0.175\n",
        "Average Accuracy:0.322256\n"
       ]
      }
     ],
     "prompt_number": 12
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# # Show the results which are not classified correctly\n",
      "def show_guess_data(classifier, test_features, test_data, number):\n",
      "    \n",
      "    # This is code from the NLTK chapter\n",
      "    errors = []\n",
      "    for idx, (message_feature, tag) in enumerate(test_features):\n",
      "        guess = classifier.classify(message_feature)\n",
      "        if guess != tag:\n",
      "            errors.append( (tag, guess, message_feature, idx) )\n",
      "            \n",
      "    print 'Error guess:%d, Correct guess:%d'% (len(errors), len(test_features) - len(errors))\n",
      "    \n",
      "#     with open('result.csv', 'wb') as fout:\n",
      "#         writer = csv.writer(fout)\n",
      "#         for (tag, guess, name) in errors:\n",
      "#             writer.writerow([tag, guess, name])\n",
      "    \n",
      "    for (tag, guess, message_feautre, idx) in errors[:number]:\n",
      "#         print 'correct=%-8s guess=%-8s name=%-30s' % (tag, guess, name)\n",
      "#         print test_data[idx][0][0]\n",
      "#         print\n",
      "#         print test_data[idx][0][1]\n",
      "#         print\n",
      "        tuples = test_data[idx][0][0] + test_data[idx][0][1]\n",
      "#         print tuples\n",
      "#         print \n",
      "\n",
      "        print 'correct=%-8s guess=%-8s data=%s' % (tag, guess, ' '.join([a for (a, b) in tuples]))\n",
      "        print\n",
      "        \n",
      "# show_guess_data(classifier, test_data)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 13
    }
   ],
   "metadata": {}
  }
 ]
}