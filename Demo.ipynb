{
 "metadata": {
  "name": "",
  "signature": "sha256:b8642baa017feebdd608acf678ec3444ff92f1f43049389bc4835a99f9b2337e"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Import statments:\n",
      "import csv\n",
      "import itertools\n",
      "import nltk\n",
      "import random\n",
      "from __future__ import division\n",
      "import time\n",
      "import cPickle as pickle \n",
      "import json\n",
      "import math\n",
      "import numpy\n",
      "\n",
      "from sklearn import cross_validation\n",
      "from collections import Counter\n",
      "from string import punctuation\n",
      "from nltk.corpus import brown"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 6
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Functions"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#Sentence and Word Tokenization\n",
      "sent_tokenizer=nltk.data.load('tokenizers/punkt/english.pickle')\n",
      "\n",
      "# #Function to build text tagger\n",
      "# def build_backoff_tagger(train_sents):\n",
      "#     t0 = nltk.DefaultTagger('NN')\n",
      "#     t1 = nltk.UnigramTagger(train_sents, backoff=t0)\n",
      "#     t2 = nltk.BigramTagger(train_sents, backoff=t1)\n",
      "#     t3 = nltk.TrigramTagger(train_sents, backoff=t2)\n",
      "#     return t3\n",
      "\n",
      "# # Train a tagger on all of the Brown corpus\n",
      "# training_data = brown.tagged_sents()\n",
      "# ngram_tagger = build_backoff_tagger(training_data)\n",
      "\n",
      "def tokenize(string):\n",
      "    tagged_sents = []\n",
      "    sentences = [nltk.word_tokenize(sent) for sent in sent_tokenizer.tokenize(string)]\n",
      "    tagged_sents = [ngram_tagger.tag(sent) for sent in sentences]\n",
      "    return tagged_sents"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 63
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "'''\n",
      "Iterates over all mail messages, then checks if it contains a cue phrase. \n",
      "It then builds a text span pair which could include the previous sentence or two parts of\n",
      "the current sentence depending on the position of the cue phrase.\n",
      "This text span will not include the cue signal.\n",
      "'''\n",
      "# mail_spans = {}\n",
      "\n",
      "def cue_phrase_finder(message):\n",
      "    mail_spans = {}\n",
      "    cue_phrases = ['so', 'because', 'since', 'per', 'thus', 'therefore', 'consequently']\n",
      "    \n",
      "    for sent_num in range(0,len(message)): # Iterate through each sent in message\n",
      "        sentence = message[sent_num]\n",
      "        # If ZERO words in sentence are in cue_phrases, sent to no_relation\n",
      "        if len([word for (word, pos) in sentence if word.lower() in cue_phrases]) == 0:\n",
      "            phrase_key = 'no_relation'\n",
      "            if phrase_key in mail_spans:\n",
      "                mail_spans[phrase_key].append((message[sent_num-1],sentence))\n",
      "            else:\n",
      "                mail_spans[phrase_key] = [(message[sent_num-1],sentence)]\n",
      "                \n",
      "        for word_num in range(0,len(sentence)): # Iterate through each word in sent\n",
      "            split = 0 # Default, if unchanged it signals cue signal was present but no cue phrase pattern\n",
      "            word = sentence[word_num][0].lower() \n",
      "\n",
      "            if word_num != 0:\n",
      "                prev_word = sentence[word_num-1][0].lower()\n",
      "            else: \n",
      "                prev_word = ''\n",
      "                \n",
      "            if word_num != (len(sentence)-1):\n",
      "                next_word = sentence[word_num+1][0].lower()\n",
      "            else:\n",
      "                next_word = ''\n",
      "\n",
      "            if word in cue_phrases:                \n",
      "                '''\n",
      "                Pull out the text phrase based on the cue phrase. Probably a \n",
      "                continuation of last sentence or split the sentence at the last \n",
      "                position of the cue phrases\n",
      "                '''\n",
      "                if sentence[-1][0] != '?':\n",
      "                    # cpp_bn_so\n",
      "                    if word_num == 0 and word == 'so':\n",
      "                        if next_word == ',':\n",
      "                            split = range(2)\n",
      "                            phrase_key = 'cpp_bn_so'\n",
      "                    # cpp_wn_per\n",
      "                    elif word_num != 0 and word == 'per':\n",
      "                        if prev_word == ',' and next_word != '=' and next_word != 'se':\n",
      "                            split = 'oneprior'\n",
      "                            phrase_key = 'cpp_wn_per'\n",
      "                    # cpp_bn_becauseofthis\n",
      "                    elif word_num == 0 and word == 'because':\n",
      "                        if ((next_word == 'of') and (sentence[word_num+2][0] == 'this')) and (sentence[word_num+3][0] == ','):\n",
      "                            split = range(4)\n",
      "                            phrase_key = 'cpp_bn_becauseofthis'\n",
      "                    # cpp_wn_because1\n",
      "                        elif (',', ',') in sentence:\n",
      "                              split = sentence.index((',', ','))\n",
      "                              phrase_key = 'cpp_wn_because1'\n",
      "                    # cpp_wn_because2\n",
      "                    elif word_num != 0 and word == 'because':\n",
      "                        if prev_word == ',':\n",
      "                            split = 'oneprior'\n",
      "                            phrase_key = 'cpp_wn_because2'\n",
      "                    # cpp_wn_because3\n",
      "                        elif word_num != len(sentence):\n",
      "                            split = 'exact'\n",
      "                            phrase_key = 'cpp_wn_because3'\n",
      "\n",
      "\n",
      "                    # cpp_wn_since\n",
      "                    elif word_num == 0 and word == 'since':\n",
      "                        if (',', ',') in sentence:\n",
      "                            split = sentence.index((',', ','))\n",
      "                            phrase_key = 'cpp_wn_since'\n",
      "                    # cpp_bn_therefore\n",
      "                    elif word_num == 0 and word == 'therefore':\n",
      "                        if next_word == ',':\n",
      "                            split = range(2)\n",
      "                            phrase_key = 'cpp_bn_therefore'\n",
      "                    # cpp_bn_consequently\n",
      "                    elif word_num == 0 and word == 'consequently':\n",
      "                        if next_word == ',':\n",
      "                            split = range(2)\n",
      "                            phrase_key = 'cpp_bn_consequently'  \n",
      "                \n",
      "                \n",
      "                '''\n",
      "                The code below splits the sentence pairs into text spans or splits the sentence itself \n",
      "                AND removes cue signals\n",
      "                '''\n",
      "                if split == 0:\n",
      "                    # Has cue word but does not follow cue phrase pattern\n",
      "                    phrase_key = 'cue_signal_unclassified'\n",
      "                    text_span = (message[sent_num-1],sentence)\n",
      "                    \n",
      "                if phrase_key[:6] == 'cpp_bn':\n",
      "                    if sent_num == 0:\n",
      "                        # Removes 21 mistaken occurences where the cue phrase extracts\n",
      "                        # the first sentence in a message as holding a discourse relation\n",
      "                        # with the sentence before it, something our classifier is not\n",
      "                        # to handle.\n",
      "                        phrase_key = 'cue_signal_unclassified'\n",
      "                        text_span = (message[sent_num-1],sentence)\n",
      "                    elif split == range(2):\n",
      "                        text_span = (message[sent_num-1],sentence)\n",
      "                    elif split == range(4):\n",
      "                        text_span = (message[sent_num-1],sentence)\n",
      "                elif phrase_key[:6] == 'cpp_wn':\n",
      "                    if split == 'oneprior':\n",
      "                        text_span = (sentence[:word_num-1],sentence[word_num-1:])\n",
      "                    elif split == 'exact':\n",
      "                        text_span = (sentence[:word_num],sentence[word_num:])\n",
      "                    else: # Ex. Because justification, claim. OR Since justification, claim. split = index of comma\n",
      "                        text_span = (sentence[:split],sentence[split:])\n",
      "\n",
      "                # Store the spans in mail_spans\n",
      "                if phrase_key in mail_spans:\n",
      "                    mail_spans[phrase_key].append(text_span)\n",
      "                else:\n",
      "                    mail_spans[phrase_key] = [text_span]\n",
      "    return mail_spans"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 70
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#Feature Extraction#"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def number_of_words_feature(features, data): \n",
      "    features['number_of_words'] = len(data[0]) + len(data[1])\n",
      "    #blah = (data[0] + data[1])\n",
      "    #features['number_of_words'] = len(set(blah))   \n",
      "    return features\n",
      "\n",
      "def has_CD_tag_feature(features, data):\n",
      "    tuples = data[0] + data[1]\n",
      "#     count = 0\n",
      "    for word, POS in tuples:\n",
      "#         if POS == 'CD':\n",
      "#             count = count + 1\n",
      "        if POS in features.keys():\n",
      "            features[POS] += 1\n",
      "        else:\n",
      "            features[POS] = 1\n",
      "\n",
      "#     features['CD'] = count        \n",
      "    return features\n",
      "\n",
      "def number_of_unique_word(features, data):\n",
      "    tuples = data[0] + data[1]\n",
      "    tokens = [tup[0] for tup in tuples]\n",
      "\n",
      "    features['number_of_unique_word'] = len(set(tokens))   \n",
      "    return features\n",
      "\n",
      "# This extracts tokens local to the cue signals.\n",
      "def local_tokens(features, data):\n",
      "    cue_phrases = ['so', 'because', 'since', 'per', 'thus', 'therefore', 'consequently']\n",
      "    tuples = data[0] + data[1]\n",
      "    tokens = [tup[0] for tup in tuples]\n",
      "    \n",
      "    for n, token in enumerate(tokens):\n",
      "        if token in cue_phrases:\n",
      "            if len(tokens) > n+1:\n",
      "                features['local1'+tokens[n+1]] = True\n",
      "            if len(tokens) > n+2:\n",
      "                features['local2'+tokens[n+2]] = True\n",
      "            break\n",
      "    return features\n",
      "\n",
      "# This simply extracts the first cue_signal.\n",
      "def first_cue_signal(features, data):\n",
      "    cue_phrases = ['so', 'because', 'since', 'per', 'thus', 'therefore', 'consequently']\n",
      "    tuples = data[0] + data[1]\n",
      "    tokens = [tup[0] for tup in tuples]\n",
      "    for n, token in enumerate(tokens):\n",
      "        if token in cue_phrases:\n",
      "            features['first_cue_signal'] = token\n",
      "            break\n",
      "    return features\n",
      "\n",
      "# This simply IDs cue signals:\n",
      "def cue_signals_second(features, data):\n",
      "    cue_phrases = ['so', 'because', 'since', 'per', 'thus', 'therefore', 'consequently']\n",
      "    tuples = data[1]\n",
      "    tokens = [tup[0] for tup in tuples]\n",
      "    for n, token in enumerate(tokens):\n",
      "        if token in cue_phrases:\n",
      "            features[token] = True\n",
      "    return features\n",
      "\n",
      "# This simply extracts the last cue_signal.\n",
      "def last_cue_signal(features, data):\n",
      "    cue_phrases = ['so', 'because', 'since', 'per', 'thus', 'therefore', 'consequently']\n",
      "    tuples = data[0] + data[1]\n",
      "    tokens = [tup[0] for tup in tuples]\n",
      "    for n, token in enumerate(tokens[::-1]):\n",
      "        if token in cue_phrases:\n",
      "            features['last_cue_signal'] = token\n",
      "            break\n",
      "    return features\n",
      "\n",
      "# This simply extracts the POS of the second data cue_signal.\n",
      "def pos_cue_signal(features, data):\n",
      "    cue_phrases = ['so', 'because', 'since', 'per', 'thus', 'therefore', 'consequently']\n",
      "    tuples = data[1]\n",
      "    for n, tup in enumerate(tuples):\n",
      "        if tup[0] in cue_phrases:\n",
      "            features['pos_cue_signal'] = tup[1]\n",
      "            break\n",
      "    return features\n",
      "\n",
      "# This simply extracts the POS of the first cue_signal.\n",
      "def first_pos_cue_signal(features, data):\n",
      "    cue_phrases = ['so', 'because', 'since', 'per', 'thus', 'therefore', 'consequently']\n",
      "    tuples = data[0] + data[1]\n",
      "    for n, tup in enumerate(tuples):\n",
      "        if tup[0] in cue_phrases:\n",
      "            features['first_pos_cue_signal'] = tup[1]\n",
      "            break\n",
      "    return features\n",
      "\n",
      "\n",
      "# This simply extracts the index of the second textspan cue_signal.\n",
      "def cue_signal_index(features, data):\n",
      "    cue_phrases = ['so', 'because', 'since', 'per', 'thus', 'therefore', 'consequently']\n",
      "    tuples = data[1]\n",
      "    tokens = [tup[0] for tup in tuples]\n",
      "    for n, token in enumerate(tokens):\n",
      "        if token in cue_phrases:\n",
      "            features['cue_signal_index'] = n\n",
      "            break\n",
      "    return features\n",
      "\n",
      "# This distinguishes only this with their second textspan longer than 90 tokens.\n",
      "def leng(features, data):\n",
      "    tuples = data[1]\n",
      "    tokens = [tup[0] for tup in tuples]\n",
      "    if len(tokens) > 90:\n",
      "        features['leng'] = 2\n",
      "    else:\n",
      "        features['leng'] = 1\n",
      "    return features\n",
      "\n",
      "def pronouns(features, data):\n",
      "    tuples = data[0] + data[1]\n",
      "    features['pronouns'] = True if len([tup[1] for tup in tuples if tup[1].startswith('PPSS')]) != 0 else False\n",
      "\n",
      "    return features\n",
      "\n",
      "def verbs(features, data):\n",
      "    tuples = data[0] + data[1]\n",
      "    features['verbs'] = True if len([tup[1] for tup in tuples if tup[1].startswith('VBZ')]) != 0 else False\n",
      "    \n",
      "    return features\n",
      "    \n",
      "def most_common_pos(features, data):\n",
      "    tuples = data[0] + data[1]\n",
      "    features['most_common'] = Counter([tup[1] for tup in tuples]).most_common(1)[0][0]\n",
      "    return features"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 82
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# TF-IDF\n",
      "def words_pos_per_category(data):\n",
      "    category_words = [[],[],[],[],[],[],[]]\n",
      "    category_pos = [[],[],[],[],[],[],[]]\n",
      "    category_tuple = [[],[],[],[],[],[],[]]\n",
      "    \n",
      "    for tup, tag in data:\n",
      "        try:\n",
      "            tuples = tup[0] + tup[1]\n",
      "            tokens_word = [tup[0] for tup in tuples]\n",
      "            tokens_pos = [tup[1] for tup in tuples]\n",
      "            category_words[tag].extend(tokens_word)\n",
      "            category_pos[tag].extend(tokens_pos)\n",
      "            category_tuple[tag].extend(tuples)\n",
      "        except:\n",
      "            pass\n",
      "    \n",
      "#     print num\n",
      "#     for each in category_words:\n",
      "#         each = [item for sublist in each for item in sublist] # To flatten the nested list of lists.\n",
      "        \n",
      "#     return category_words\n",
      "    return category_words, category_pos, category_tuple\n",
      "\n",
      "def FreqDists_per_category(category_term):\n",
      "    categoryFreqDists = []\n",
      "    for category in category_term:\n",
      "        categoryFreqDists.append(nltk.FreqDist([w for w in category]))\n",
      "#         if isinstance(category[0], str):\n",
      "#             categoryFreqDists.append(nltk.FreqDist([w for w in category]))\n",
      "#         else:\n",
      "#             categoryFreqDists.append(nltk.FreqDist([w for w in category]))\n",
      "    return categoryFreqDists\n",
      "\n",
      "def get_general_tfidf(categories, categoryWordFreqDists, categoryPOSFreqDists, categoryTupleFreqDists):\n",
      "    '''tf.idf  equation from week6b_hearst_anlp2014.pdf, code written by Daniel'''\n",
      "    tfidf_word = []\n",
      "    tfidf_pos = []\n",
      "    tfidf_tuple = []\n",
      "    for n, each in enumerate(categories):\n",
      "        for key in categoryWordFreqDists[n].keys():#[:275]: # This was previously set to [:100]\n",
      "                tf = categoryWordFreqDists[n][key]/len(categoryWordFreqDists[n].keys())\n",
      "                D = 0\n",
      "                for n2, each2 in enumerate(categories):\n",
      "                    if key in categoryWordFreqDists[n2].keys():\n",
      "                        D += 1\n",
      "                IDF = math.log(len(categories)/D)\n",
      "                tf_IDF = math.log((1+tf))*IDF\n",
      "                tfidf_word.append((n+1, key, tf_IDF))\n",
      "#     sorted_general_tfidf = sorted(tfidf,key=lambda x: x[2], reverse = True) # Sort from largest to smallest.   \n",
      "#     return sorted_general_tfidf\n",
      "\n",
      "        for key in categoryPOSFreqDists[n].keys():#[:275]: # This was previously set to [:100]\n",
      "                tf = categoryPOSFreqDists[n][key]/len(categoryPOSFreqDists[n].keys())\n",
      "                D = 0\n",
      "                for n2, each2 in enumerate(categories):\n",
      "                    if key in categoryPOSFreqDists[n2].keys():\n",
      "                        D += 1\n",
      "                IDF = math.log(len(categories)/D)\n",
      "                tf_IDF = math.log((1+tf))*IDF\n",
      "                tfidf_pos.append((n+1, key, tf_IDF))\n",
      "                \n",
      "        for key in categoryTupleFreqDists[n].keys():#[:275]: # This was previously set to [:100]\n",
      "                tf = categoryTupleFreqDists[n][key]/len(categoryTupleFreqDists[n].keys())\n",
      "                D = 0\n",
      "                for n2, each2 in enumerate(categories):\n",
      "                    if key in categoryTupleFreqDists[n2].keys():\n",
      "                        D += 1\n",
      "                IDF = math.log(len(categories)/D)\n",
      "                tf_IDF = math.log((1+tf))*IDF\n",
      "                tfidf_tuple.append((n+1, key, tf_IDF))\n",
      "    \n",
      "    sorted_general_word_tfidf = sorted(tfidf_word,key=lambda x: x[2], reverse = True) # Sort from largest to smallest.\n",
      "    sorted_general_pos_tfidf = sorted(tfidf_pos,key=lambda x: x[2], reverse = True) # Sort from largest to smallest.\n",
      "    sorted_general_tuple_tfidf = sorted(tfidf_tuple,key=lambda x: x[2], reverse = True) # Sort from largest to smallest.\n",
      "    \n",
      "    return sorted_general_word_tfidf, sorted_general_pos_tfidf, sorted_general_tuple_tfidf\n",
      "\n",
      "def top_general_tfidf(sorted_general_tfidf, size):\n",
      "    top = []\n",
      "    for tag, token, freq in sorted_general_tfidf[:size]: top.append(token)\n",
      "    return set(top)\n",
      "\n",
      "def prep_bulk_features(train):\n",
      "    category_words, category_pos, category_tuple = words_pos_per_category(train)\n",
      "    categoryWordFreqDists = FreqDists_per_category(category_words)\n",
      "    categoryPOSFreqDists = FreqDists_per_category(category_pos)\n",
      "    categoryTupleFreqDists = FreqDists_per_category(category_tuple)\n",
      "#     sorted_general_tfidf = get_general_tfidf(categories, categoryFreqDists)\n",
      "#     return sorted_general_tfidf\n",
      "\n",
      "    sorted_general_word_tfidf, sorted_general_pos_tfidf, sorted_general_tuple_tfidf = \\\n",
      "        get_general_tfidf(categories, categoryWordFreqDists, categoryPOSFreqDists, categoryTupleFreqDists)\n",
      "        \n",
      "    return sorted_general_word_tfidf, sorted_general_pos_tfidf, sorted_general_tuple_tfidf\n",
      "\n",
      "def tfidf_feature(features, data, word_tfidf, pos_tfidf, tuple_tfidf, option):\n",
      "    \"\"\"\"This pulls features from the records, line by line, and compares them with three \n",
      "    preprocessed sets of items created through identifying tf.idf for the items.\"\"\"\n",
      "\n",
      "    try:\n",
      "        tuples = data[0] + data[1]\n",
      "#         if option == 'word' : \n",
      "#             print 'option2:%s' % option\n",
      "        if option == 'pos':\n",
      "            tokens = [tup[1] for tup in tuples]\n",
      "            tfidf = pos_tfidf\n",
      "            threshold = 4\n",
      "            \n",
      "            \n",
      "        elif option == 'word' : \n",
      "            tokens = [tup[0] for tup in tuples]\n",
      "            tfidf = word_tfidf\n",
      "            threshold = 120\n",
      "        else:\n",
      "            tokens = tuples\n",
      "            tfidf = tuple_tfidf\n",
      "            threshold = 200  \n",
      "        \n",
      "        for n, each in enumerate(tokens):\n",
      "            for term in top_general_tfidf(tfidf, threshold):\n",
      "                if term == each:\n",
      "                    features[term] = 1\n",
      "    except:\n",
      "        print 'exception'\n",
      "    return features"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 83
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def full_feature_extraction(data, word_tfidf, pos_tfidf, tuple_tfidf):\n",
      "    features = {}\n",
      "    \n",
      "#     features = cartesian_product_feature(features, data)\n",
      "#     features = number_of_words_feature(features, data)\n",
      "#     features = tfidf_feature(features, data, word_tfidf, pos_tfidf, tuple_tfidf, 'word')\n",
      "    features = tfidf_feature(features, data, word_tfidf, pos_tfidf, tuple_tfidf, 'pos')\n",
      "    features = tfidf_feature(features, data, word_tfidf, pos_tfidf, tuple_tfidf, 'tuple')\n",
      "#     features = has_CD_tag_feature(features, data)\n",
      "#     features = number_of_unique_word(features, data)\n",
      "\n",
      "    features = local_tokens(features, data)\n",
      "    features = first_cue_signal(features, data)\n",
      "    features = cue_signals_second(features, data)\n",
      "    features = last_cue_signal(features, data)\n",
      "    features = pos_cue_signal(features, data)\n",
      "    features = first_pos_cue_signal(features, data)\n",
      "    features = cue_signal_index(features, data)\n",
      "    features = leng(features, data)\n",
      "    \n",
      "#     features = pronouns(features, data)\n",
      "#     features = verbs(features, data)\n",
      "\n",
      "\n",
      "#     features = most_common_pos(features, data)\n",
      "    \n",
      "    return features\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 84
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Naive Bayes Classifer"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Training Data"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "with open('justification_tags.json', 'rb') as file:\n",
      "    data = eval(file.read())"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 135
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "hand_coded_data = list()\n",
      "for x in data:\n",
      "    lst = list()\n",
      "    lst.append(([(z[0], z[1]) for z in x['sent']],[(z[0], z[1]) for z in x['prev']]))\n",
      "    lst.append(x['justification'])\n",
      "    hand_coded_data.append(lst)\n",
      "\n",
      "types = list()\n",
      "for x in hand_coded_data:\n",
      "    if x[1] not in types:\n",
      "        types.append(x[1])\n",
      "categories = dict(enumerate(types, start=0))\n",
      "\n",
      "hand_coded_data = [(tup, types.index(typ)) for (tup, typ) in hand_coded_data]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 89
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Train classifier"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def nb_classifier(data):\n",
      "    train_features = [(full_feature_extraction(tup, word_tfidf, pos_tfidf, tuple_tfidf), tag) for (tup, tag) in data]\n",
      "    nb_classifier = nltk.NaiveBayesClassifier.train(train_features)\n",
      "    return nb_classifier"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 103
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "word_tfidf, pos_tfidf, tuple_tfidf = prep_bulk_features(hand_coded_data)\n",
      "nb_classifier = nb_classifier(hand_coded_data)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 104
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def predict(message):\n",
      "    output = tokenize(message)\n",
      "\n",
      "    mail_spans = cue_phrase_finder(output)\n",
      "    to_predict = [tup for key in mail_spans for tup in mail_spans[key] if key != 'no_relation']\n",
      "\n",
      "    print '----------------------\\nOutput\\n----------------------'\n",
      "    if to_predict:\n",
      "\n",
      "        for pattern in to_predict:\n",
      "            test_features = full_feature_extraction(pattern, word_tfidf, pos_tfidf, tuple_tfidf)\n",
      "            result = nb_classifier.classify(test_features)\n",
      "            print 'Found {0} justification'.format(categories[result])\n",
      "    else:\n",
      "        print 'No justification in message.'"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 132
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#Try it out for yourself!\n",
      "## Enter a couple sentences below and see if we can predict the type of justification claim:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "while 1:\n",
      "    message = raw_input(\"Test sentence: \")\n",
      "    predict(message)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "name": "stdout",
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Test sentence: the exhbition is closed because of the rain\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "----------------------\n",
        "Output\n",
        "----------------------\n",
        "Found OTHER justification\n"
       ]
      }
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}